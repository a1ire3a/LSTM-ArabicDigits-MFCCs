# -*- coding: utf-8 -*-
"""DL_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v6LXtx_uoUbzWkl_ldEZS9L4BA5zJIW6
"""

import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Embedding
from sklearn.utils import shuffle
from numpy.core.fromnumeric import mean
import timeit
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

"""# **READ DATA**"""

def load_datasets(path, num):
    f = open(path)
    temp_data = []
    temp_label = []
    number = []
    label = -1
    count = 0

    while True:
        line = f.readline()
        temp = np.array(line.split(), dtype=float)
        if temp.size != 0: 
            number.append(temp)
        else:
            if np.array(number).size != 0:
                if count % num == 0:
                    label +=1
                temp_data.append(np.array(number, dtype=float))
                temp_label.append(label)
                count += 1
            number.clear()

        if not line:
            temp_data = np.array(temp_data)
            temp_label = np.array(temp_label)
            break
    return(temp_data, temp_label)

path_train = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/Data/Train_Arabic_Digit.txt'
path_test = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/Data/Test_Arabic_Digit.txt'
train_data, train_label = load_datasets(path_train, 660)
test_data, test_label = load_datasets(path_test, 220)

"""# **BATCH GENERATOR**"""

def batch_gen(X,y,batch_number):
    X_batches = []
    X_split = np.split(X, batch_number)
    y_batches = np.split(y, batch_number)
    for i in X_split:
        X_batches.append(tf.keras.preprocessing.sequence.pad_sequences(i, padding="post", dtype=float)) #padding
    return X_batches, y_batches

#shuffle dataset
X_train, y_train = shuffle(train_data, train_label)
#create batch 
batch_length = 100
batch_num = int(len(X_train)/batch_length)
X, y = batch_gen(X_train, y_train, batch_num)

X_test, y_test = shuffle(test_data, test_label)
batch_num_test = int(len(X_test)/batch_length)
Xtest, ytest = batch_gen(X_train, y_train, batch_num_test)

"""# **CREATE MODEL**"""

model_lstm = Sequential()

model_lstm.add(LSTM(units=512, stateful=False, input_shape=(None, 13),dropout = 0.3, recurrent_dropout = 0.3))
model_lstm.add(Dense(256, activation='relu'))
model_lstm.add(Dropout(0.3))
#model_lstm.add(Dense(64, activation='relu'))
model_lstm.add(Dense(10, activation = 'softmax'))
model_lstm.summary()

model_lstm.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer='Adam',
    metrics=['accuracy']
)

"""**run model**"""

rep = []
for i in range(batch_num):
	rep.append(model_lstm.fit(
    X[i],
    y[i],
    validation_split=0.1,
    epochs = 10
))
	model_lstm.reset_states()

res = []
for i in range(batch_num_test):
	res.append(model_lstm.evaluate(
    Xtest[i],
    ytest[i],
))

"""# **RESULT**"""

res = np.array(res)
loss = res[:,0]
acc = res[:,1]
print('loss = ', mean(loss))
print('accuracy = ', mean(acc))

print('training epoch = ', 10 * batch_num)
print('batch size = ',batch_length)

acc_rep = rep[0].history['accuracy']
for i in range (len(rep)-1):
    acc_rep += rep[i+1].history['accuracy']
Vacc_rep = rep[0].history['val_accuracy']
for i in range (len(rep)-1):
    Vacc_rep += rep[i+1].history['val_accuracy']

plt.plot(acc_rep, label='accuracy')
plt.plot(Vacc_rep, label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')

loss_rep = rep[0].history['loss']
for i in range (len(rep)-1):
    loss_rep += rep[i+1].history['loss']
Vloss_rep = rep[0].history['val_loss']
for i in range (len(rep)-1):
    Vloss_rep += rep[i+1].history['val_loss']

plt.plot(loss_rep, label='loss')
plt.plot(Vloss_rep, label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 4])
plt.legend(loc='lower right')

"""# **TRY FOR DEFFERENT BATCH AND EPOCH**

**1/5**
"""

#shuffle dataset
X_train, y_train = shuffle(train_data, train_label)
#create batch 
batch_length = 100
batch_num = int(len(X_train)/batch_length)
X, y = batch_gen(X_train, y_train, batch_num)

X_test, y_test = shuffle(test_data, test_label)
batch_num_test = int(len(X_test)/batch_length)
Xtest, ytest = batch_gen(X_train, y_train, batch_num_test)

print('training epoch = ', 5 * batch_num)
print('batch size = ',batch_length)

start = timeit.default_timer()
rep = []
for i in range(batch_num):
	rep.append(model_lstm.fit(
    X[i],
    y[i],
    validation_split=0.1,
    epochs = 5
))
	model_lstm.reset_states()

stop = timeit.default_timer()

res = []
for i in range(batch_num_test):
	res.append(model_lstm.evaluate(
    Xtest[i],
    ytest[i],
))

res = np.array(res)
loss = res[:,0]
acc = res[:,1]
print('loss = ', mean(loss))
print('accuracy = ', mean(acc))
print('Time = ', stop - start)

acc_rep = rep[0].history['accuracy']
for i in range (len(rep)-1):
    acc_rep += rep[i+1].history['accuracy']
Vacc_rep = rep[0].history['val_accuracy']
for i in range (len(rep)-1):
    Vacc_rep += rep[i+1].history['val_accuracy']
plt.plot(acc_rep, label='accuracy')
plt.plot(Vacc_rep, label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')

loss_rep = rep[0].history['loss']
for i in range (len(rep)-1):
    loss_rep += rep[i+1].history['loss']
Vloss_rep = rep[0].history['val_loss']
for i in range (len(rep)-1):
    Vloss_rep += rep[i+1].history['val_loss']
plt.plot(loss_rep, label='loss')
plt.plot(Vloss_rep, label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 4])
plt.legend(loc='lower right')

"""**2/5**"""

#shuffle dataset
X_train, y_train = shuffle(train_data, train_label)
#create batch 
batch_length = 200
batch_num = int(len(X_train)/batch_length)
X, y = batch_gen(X_train, y_train, batch_num)

X_test, y_test = shuffle(test_data, test_label)
batch_num_test = int(len(X_test)/batch_length)
Xtest, ytest = batch_gen(X_train, y_train, batch_num_test)

print('training epoch = ', 10 * batch_num)
print('batch size = ',batch_length)

start = timeit.default_timer()
rep = []
for i in range(batch_num):
	rep.append(model_lstm.fit(
    X[i],
    y[i],
    validation_split=0.1,
    epochs = 10
))
	model_lstm.reset_states()

stop = timeit.default_timer()

res = []
for i in range(batch_num_test):
	res.append(model_lstm.evaluate(
    Xtest[i],
    ytest[i],
))

res = np.array(res)
loss = res[:,0]
acc = res[:,1]
print('loss = ', mean(loss))
print('accuracy = ', mean(acc))
print('Time = ', stop - start)

acc_rep = rep[0].history['accuracy']
for i in range (len(rep)-1):
    acc_rep += rep[i+1].history['accuracy']
Vacc_rep = rep[0].history['val_accuracy']
for i in range (len(rep)-1):
    Vacc_rep += rep[i+1].history['val_accuracy']
plt.plot(acc_rep, label='accuracy')
plt.plot(Vacc_rep, label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')

loss_rep = rep[0].history['loss']
for i in range (len(rep)-1):
    loss_rep += rep[i+1].history['loss']
Vloss_rep = rep[0].history['val_loss']
for i in range (len(rep)-1):
    Vloss_rep += rep[i+1].history['val_loss']
plt.plot(loss_rep, label='loss')
plt.plot(Vloss_rep, label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 4])
plt.legend(loc='lower right')

"""**3/5**"""

#shuffle dataset
X_train, y_train = shuffle(train_data, train_label)
#create batch 
batch_length = 200
batch_num = int(len(X_train)/batch_length)
X, y = batch_gen(X_train, y_train, batch_num)

X_test, y_test = shuffle(test_data, test_label)
batch_num_test = int(len(X_test)/batch_length)
Xtest, ytest = batch_gen(X_train, y_train, batch_num_test)

print('training epoch = ', 5 * batch_num)
print('batch size = ',batch_length)

start = timeit.default_timer()
rep = []
for i in range(batch_num):
	rep.append(model_lstm.fit(
    X[i],
    y[i],
    validation_split=0.1,
    epochs = 5
))
	model_lstm.reset_states()

stop = timeit.default_timer()

res = []
for i in range(batch_num_test):
	res.append(model_lstm.evaluate(
    Xtest[i],
    ytest[i],
))

res = np.array(res)
loss = res[:,0]
acc = res[:,1]
print('loss = ', mean(loss))
print('accuracy = ', mean(acc))
print('Time = ', stop - start)

acc_rep = rep[0].history['accuracy']
for i in range (len(rep)-1):
    acc_rep += rep[i+1].history['accuracy']
Vacc_rep = rep[0].history['val_accuracy']
for i in range (len(rep)-1):
    Vacc_rep += rep[i+1].history['val_accuracy']
plt.plot(acc_rep, label='accuracy')
plt.plot(Vacc_rep, label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')

loss_rep = rep[0].history['loss']
for i in range (len(rep)-1):
    loss_rep += rep[i+1].history['loss']
Vloss_rep = rep[0].history['val_loss']
for i in range (len(rep)-1):
    Vloss_rep += rep[i+1].history['val_loss']
plt.plot(loss_rep, label='loss')
plt.plot(Vloss_rep, label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 4])
plt.legend(loc='lower right')

"""**4/5**"""

#shuffle dataset
X_train, y_train = shuffle(train_data, train_label)
#create batch 
batch_length = 550
batch_num = int(len(X_train)/batch_length)
X, y = batch_gen(X_train, y_train, batch_num)

X_test, y_test = shuffle(test_data, test_label)
batch_num_test = int(len(X_test)/batch_length)
Xtest, ytest = batch_gen(X_train, y_train, batch_num_test)

print('training epoch = ', 5 * batch_num)
print('batch size = ',batch_length)

start = timeit.default_timer()
rep = []
for i in range(batch_num):
	rep.append(model_lstm.fit(
    X[i],
    y[i],
    validation_split=0.1,
    epochs = 5
))
	model_lstm.reset_states()

stop = timeit.default_timer()

res = []
for i in range(batch_num_test):
	res.append(model_lstm.evaluate(
    Xtest[i],
    ytest[i],
))

res = np.array(res)
loss = res[:,0]
acc = res[:,1]
print('loss = ', mean(loss))
print('accuracy = ', mean(acc))
print('Time = ', stop - start)

acc_rep = rep[0].history['accuracy']
for i in range (len(rep)-1):
    acc_rep += rep[i+1].history['accuracy']
Vacc_rep = rep[0].history['val_accuracy']
for i in range (len(rep)-1):
    Vacc_rep += rep[i+1].history['val_accuracy']
plt.plot(acc_rep, label='accuracy')
plt.plot(Vacc_rep, label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')

loss_rep = rep[0].history['loss']
for i in range (len(rep)-1):
    loss_rep += rep[i+1].history['loss']
Vloss_rep = rep[0].history['val_loss']
for i in range (len(rep)-1):
    Vloss_rep += rep[i+1].history['val_loss']
plt.plot(loss_rep, label='loss')
plt.plot(Vloss_rep, label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 4])
plt.legend(loc='lower right')

"""**5/5**"""

#shuffle dataset
X_train, y_train = shuffle(train_data, train_label)
#create batch 
batch_length = 550
batch_num = int(len(X_train)/batch_length)
X, y = batch_gen(X_train, y_train, batch_num)

X_test, y_test = shuffle(test_data, test_label)
batch_num_test = int(len(X_test)/batch_length)
Xtest, ytest = batch_gen(X_train, y_train, batch_num_test)

print('training epoch = ', 20 * batch_num)
print('batch size = ',batch_length)

start = timeit.default_timer()
rep = []
for i in range(batch_num):
	rep.append(model_lstm.fit(
    X[i],
    y[i],
    validation_split=0.1,
    epochs = 20
))
	model_lstm.reset_states()

stop = timeit.default_timer()

res = []
for i in range(batch_num_test):
	res.append(model_lstm.evaluate(
    Xtest[i],
    ytest[i],
))

res = np.array(res)
loss = res[:,0]
acc = res[:,1]
print('loss = ', mean(loss))
print('accuracy = ', mean(acc))
print('Time = ', stop - start)

acc_rep = rep[0].history['accuracy']
for i in range (len(rep)-1):
    acc_rep += rep[i+1].history['accuracy']
Vacc_rep = rep[0].history['val_accuracy']
for i in range (len(rep)-1):
    Vacc_rep += rep[i+1].history['val_accuracy']
plt.plot(acc_rep, label='accuracy')
plt.plot(Vacc_rep, label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')

loss_rep = rep[0].history['loss']
for i in range (len(rep)-1):
    loss_rep += rep[i+1].history['loss']
Vloss_rep = rep[0].history['val_loss']
for i in range (len(rep)-1):
    Vloss_rep += rep[i+1].history['val_loss']
plt.plot(loss_rep, label='loss')
plt.plot(Vloss_rep, label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim([0, 4])
plt.legend(loc='lower right')
